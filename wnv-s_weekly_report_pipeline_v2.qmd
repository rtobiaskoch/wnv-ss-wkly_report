---
title: "wnv-s_weekly_reports"
format: html
editor: visual
execute:
  echo: false  # Hides code globally but keeps output
editor_options: 
  markdown: 
    wrap: 100
project:
  log-dir:logs
---

# Weekly Data Report

### CREATE CONFIGURATION

----------------------------------------------------------------------------------------------------

DESC: defines filepaths \| \| defines settings \| \| defines filenames \| \| creates setting for
plots\
INPUT: user arguments\
OUTPUT: config/config_weekly_settings/config_weekly\_\[sys.time\].RData

Example Run in run_config.sh

Rscript config/config_weekly.R --input 1_input/w25 --year 2025 --week 25 --download T --update T
--push F

### LOAD CONFIGURATION

----------------------------------------------------------------------------------------------------

DESC: loads in packages \| \| loads user functions \| \| loads config file

```{r, load config}
rm(list = ls())
getwd()

source("config/load_packages.R")

utils = list.files(path = "utils",
                   pattern = "*.R",
                   full.names = T)


purrr::walk(utils, source)

check_files_date("config/config_weekly_settings")
#get the most recent configuration for the run from the output of running config_weekly.R
read_latest("config/config_weekly_settings")


```

### DATA GDRIVE LOAD

----------------------------------------------------------------------------------------------------

DESC: reads in current wnv-s_database \| \| foco_trap \| \| key_rename \| \| standards data to
dir_input\
**DEPENDENCIES:** googlesheets4 \| \| googledrive \| \| argparse \| \| gsheet_pull_prompt\
**INPUT:** NA\
**OUTPUT:** **\[dir_input\]/wnv-s_database.csv** \| \| **\[dir_input\]/foco_trap.csv\
\[dir_input\]/standards_input.csv** \| \| **\[dir_input\]/key_rename.csv**\

```{r, gdrive-download}
if(download) {

#WNV DATABASE
gsheet_pull_prompt(fn_gdrive_database, "data", key_database_gsheet)
#SLEV DATABASE
gsheet_pull_prompt(fn_gdrive_database_slev, "data", key_database_gsheet_slev)
#CULEX SHEET
gsheet_pull_prompt(fn_gdrive_culex_sheet, "data", key_database_culex_sheet)
#RENAMING KEY
gsheet_pull_prompt(fn_key_rename, "Sheet1", key_rename_key)
#FOCO TRAP
gsheet_pull_prompt(fn_trap, "data", key_foco_trap)
#STANDARDS
gsheet_pull_prompt(fn_standards, "data", key_standards_gsheet)
#INCONCLUSIVE
gsheet_pull_prompt(fn_inconclusive, "data", key_inconclusive)

}



df_key_rename = read.csv(fn_key_rename)  
foco_trap = read.csv(fn_trap)  %>% filter(active == 1)
standards = read.csv(fn_standards)
```

### DATABASE CHECK

----------------------------------------------------------------------------------------------------

```{r, database-check, include=FALSE}
#read in database WNV
database = read.csv(fn_gdrive_database) %>% 
  wnv_s_clean(distinct_col = c("csu_id", "trap_id", "trap_date", "year", "week", "spp"))

check_data(df = database, #data you want to check
           trap = foco_trap, #data with trap_ids you want to ensure traps are available
           dir = dir_mid)
```

```{r, slev-database-check, include=FALSE}
#read in database SLEV
database_slev = read.csv(fn_gdrive_database_slev) %>% 
  wnv_s_clean()

check_data(df = database_slev, #data you want to check
           trap = foco_trap, #data with trap_ids you want to ensure traps are available
           dir = dir_mid)
```

```{r, culex-database-check, include=FALSE}
culex_database = read.csv(fn_gdrive_culex_sheet) %>%
 # filter(trap_status != "missing") %>% #TO DO add to wnv_s_clean and remove
  wnv_s_clean()

check_data(df = culex_database, #data you want to check
           trap = foco_trap, #data with trap_ids you want to ensure traps are available
           dir = dir_mid)

```

### DATASHEET CLEAN

----------------------------------------------------------------------------------------------------

Readme DESC: - reads in datasheet (pool) data from VDCI and BC, - cleans it into the proper format
for the database - checks each column for errors. -

DEPENDENCIES: read_list \| \| key rename \| \| wnv_s_clean \| \| tidyverse \| \| purrr

INPUT: \[dir_input\]/datasheets\* \| \| df_key_rename

OUTPUT: \[mid_dir\]/\[fn_datasheet_clean\]\*

```{r, datasheet-clean, include=FALSE}
datasheet_input = read_list(dir_datasheet, "*")
saveRDS(datasheet_input, fn_weekly_input_format_mid)

datasheet_clean = datasheet_input %>%
    key_rename(rename_df = df_key_rename, 
             drop_extra = T) %>%
    wnv_s_clean()

check_data(df = datasheet_clean, #data you want to check
           trap = foco_trap, #data with trap_ids you want to ensure traps are available
           dir = dir_mid,
           year_filter = year_filter, # The year the dates should be 
           week_filter = week_filter)
```

### PCR CLEAN

----------------------------------------------------------------------------------------------------

DESC: Reads in the pcr data from quantstudio that contains the Ct values \| \| reads in platemap
data that links the platmap position with sample id\
\
DEPENDENCIES: tidyverse \| \| readxl \| \| purrr

INPUT -PCR: \[dir_input\]/pcr/\* \[dir_pcr\] \| \| PLATEMAP: \[dir_input\]/platemap/\*
\[dir_platemap\] \| \| week_filter = 37 \| \| year_filter = 2024 \| \| copy_threshold = 500 \| \|
rn_threshold = 34000

OUTPUT - Cq combined pcr and platemap csv \[fn_cq_out = 2_mid/y##\_w##\_platemap.csv"\]

```{r,pcr-clean, include=F}
#TO DO make a function
#source("0_R/2_pcr_platemap_read_clean.R")

fn_path = list.files(path = dir_pcr, #
               full.names = T,
               ignore.case = T) 

cat("Reading in the PCR file: ", fn_path)

pcr_input = read_pcr(fn_path, sheet = "Results")

pcr_clean = clean_pcr(pcr_input,
                      y_pattern = "(?<=y)\\d+", #pcr filename pattern that determines year
                      w_pattern = "(?<=w)\\d+", #pcr filename pattern that determines week
                      p_pattern = "(?<=p)\\d+", #pcr filename pattern that determines plate#
                      undet_val = '55.55') #numeric value to replace "Undetermined" to ensure that the column is numeric without missing values (NA)

check_data(df = pcr_clean, 
           trap = foco_trap, 
           dir = dir_mid,
           year_filter = year_filter, 
           week_filter = week_filter)

write_rds(pcr_clean, paste0(dir_mid_wk, "pcr_clean.RData"))
rm(fn_path)
```

### PLATEMAP CLEAN

----------------------------------------------------------------------------------------------------

DESC:\
DEPENDENCIES:\
INPUT:\
OUTPUT:

```{r,platemap-clean, include=FALSE}
fn_path = list.files(path = dir_platemap,
                     full.names = T,
                     ignore.case = T)  

cat("\nReading in the following platemap files:\n", 
    paste(unlist(fn_path), collapse = "\n"))

platemap_input = read_platemap(fn_path,
                      sheet = "pcr", #sheet in excel file that contains platemap
                      range = "A1:M9", #range in the sheet that contains your platemap
                      col_pattern = "(?<=x)\\d+", #pattern in col with column number
                      val_to = "csu_id" #column name you want to identify your sample
                          )
platemap_clean = clean_platemap(platemap_input,
                      y_pattern = "(?<=y)\\d+", #pcr filename pattern that determines year
                      w_pattern = "(?<=w)\\d+", #pcr filename pattern that determines week
                      p_pattern = "(?<=p)\\d+" #pcr filename pattern that determines plate#
                                )

check_data(df = platemap_clean, 
           trap = foco_trap, 
           dir = dir_mid,
           year_filter = year_filter, 
           week_filter = week_filter)

write_rds(platemap_clean, paste0(dir_mid_wk, "platemap_clean.RData"))

rm(fn_path)
```

### MERGE PCR & PLATEMAP DEFINE POSITIVES

----------------------------------------------------------------------------------------------------

```{r,merge-pcr-platemap, include=FALSE}
cq_data = merge_pcr_platemap(pcr_clean, platemap_clean) %>%
  wnv_s_clean()

  #alternate test_code
inconclusive = read.csv(fn_inconclusive)


cq_data = cq_data %>%
     mutate(test_code = if_else(csu_id %in% inconclusive$csu_id, 0, test_code))

check_data(df = cq_data, 
           trap = foco_trap, 
           dir = dir_mid,
           year_filter = year_filter, 
           week_filter = week_filter,
           copy_threshold = copy_threshold)

write_rds(cq_data, paste0(dir_mid_wk, "cq_data.RData"))
```

### STANDARDS

----------------------------------------------------------------------------------------------------

```{r, standards, include=FALSE}
std_new = clean_standards(cq_data)

if(update){
  update_gsheet(new = std_new, 
              old = standards, 
              by = c("year", "week", "plate", "target", "sample_type", "csu_id"), 
              fn_save = fn_standards, 
              gkey = key_standards_gsheet,
              gfolder = "database_archive", 
              gname = "standards_archive", 
              col_database = names(standards), 
              sheet = "data" )
}

```

### BIRDS

----------------------------------------------------------------------------------------------------

```{r, birds, include=FALSE}
#add birds 
birds = cq_data %>%
  filter(sample_type == "bird") %>%
  mutate(year = as.double(year),
         week = as.double(week)) %>% #for natural merge
  select(csu_id, year, week, target_name, test_code, cq, copies, amp_status)



if(update){
  update_gsheet(new = birds, 
              #old = standards, 
              by = c("csu_id", "year", "week", "target_name"), 
              fn_save = fn_bird_output, 
              gkey = key_birds,
              gfolder = "database_archive", 
              gname = "birds_archive", 
              col_database = names(birds), 
              sheet = "data" )
}
#rm(birds)
```

### PCR PLOT

----------------------------------------------------------------------------------------------------

```{r, pcr plot}
standards = distinct_all(standards) #temporay having issues with duplicates

std = bind_rows(std_new, standards) %>%
  filter(year == year_filter)

p_std = plot_std(std)

p_pcr_wnv = plot_pcr(cq_data, 
                 virus = "WNV", 
                 pattern_2_keep = "WNV|CSU|RMRP|CDC|pos|neg",
                 copy_threshold = copy_threshold, 
                 week_filter = week_filter)



p_pcr_slev = plot_pcr(cq_data, 
                 virus = "SLEV", 
                 pattern_2_keep = "SLEV|CSU|RMRP|CDC|pos|neg",
                 copy_threshold = copy_threshold, 
                 week_filter = week_filter)

pcr_plot = p_std / p_pcr_wnv /  p_pcr_slev
pcr_plot




ggsave(fn_pcr_plot, pcr_plot,
       width = 8, height = 8, units = "in")

# Upload a single file to a specific folder
drive_upload(
  media = fn_pcr_plot,
  path = as_id(key_plots_dir),  # Replace with your folder ID
  name = basename(fn_pcr_plot)       # Optional: specify a new name
)


rm(p_std, p_pcr_wnv, p_pcr_slev)
#rm(standards, cq_data)
```

### DATABASE UPDATE

----------------------------------------------------------------------------------------------------

DESC: MERGE WNV DATASHEET & PCR

```{r, database-update, include=FALSE}

#causing duplicates
database_new = merge_4_database(datasheet_clean, cq_data,   
                            virus = "WNV",
                            copy_threshold = copy_threshold) %>%
  wnv_s_clean(distinct_col = c("csu_id", "trap_id", "year", "week", "spp"))


write_rds(database_new, paste0(dir_mid_wk, "database_new.RData"))

if(update){
  update_gsheet(new = database_new,
              old = database,
              by = c("csu_id", "trap_id", "trap_date", "spp", "zone"),
              fn_save = fn_database_update,
              gkey = key_database_gsheet,
              gfolder = "database_archive",
              gname = "wnv-s_database_archive",
              col_database = col_database,
              sheet = "data")
}


```

### MERGE SLEV DATASHEET & PCR

----------------------------------------------------------------------------------------------------

```{r slev-update, include=FALSE}

database_slev_new = merge_4_database(datasheet_clean, cq_data,   
                            virus = "SLEV",
                            copy_threshold = copy_threshold) %>%
  wnv_s_clean(distinct_col = c("csu_id", "trap_id", "year", "week", "spp"))

if(update){
  update_gsheet(new = database_slev_new,
              old = database_slev,
              by = c("csu_id", "trap_id", "trap_date", "spp", "zone"),
              fn_save = fn_database_slev_update,
              gkey = key_database_gsheet_slev,
              gfolder = "database_archive",
              gname = "slev-s_database_archive",
              col_database = names(database_slev_new),
              sheet = "data")
}

```

Code Below:

-   data_input:
    -   datasheets
    -   pcr_sheet
    -   platemap
    -   fn_func_trap: data_mid/functional_traps.csv \*to be added
-   data_output:
    -   fn_database_output
    -   unmatched traps
-   description:
    -   checks to ensure traps match existing list
    -   pulls database
    -   merges datasheet pcr and platemap data
    -   makes archive copy of database
    -   merges new data with database
    -   pushes changes from new data to database to googledrive

Code Below: -data_input

```{r weekly-format-export, include=FALSE}
#source("0_R/check_read_fun.R")
#source("0_R/weekly_data_input.R")


weekly_data_format = clean_4_weekly_input(fn_weekly_input_format_mid, database_new)

write.csv(weekly_data_format, fn_weekly_input_format, row.names = F, na = "")
```

### CLEAN APP SPECIES (TRAP) DATA

----------------------------------------------------------------------------------------------------

DESC:\
DEPENDENCIES:\
INPUT:\
OUTPUT:

```{r, clean-all-spp, include=FALSE}
#TO DO add 
all_spp0 = read_list(dir_all_spp, "*") 
all_spp = all_spp0 %>%
  key_rename(rename_df = df_key_rename,
             drop_extra = T) %>%
   mutate(trap_date = lubridate::parse_date_time(trap_date, 
                                                 orders = c("ymd", "mdy", "dmy", "Ymd"),
                                                 quiet = TRUE),
        trap_date = as.Date(trap_date)) %>%
  mutate(spp0 = spp) %>% #keep original
  wnv_s_clean(rm_col = c("trap_date"))

write_rds(all_spp, paste0(dir_mid_wk,"all_species_active_trap.RData"))

```

\

### COMPLETE TRAP DATA

----------------------------------------------------------------------------------------------------

DESC:\
DEPENDENCIES:\
INPUT:\
OUTPUT:

```{r, clean-update-all-spp-culex, include=FALSE}
#add in the trap data for the pools they send

#keep only traps in routine active traps from surveillance
culex_new = all_spp %>%
  filter(trap_id %in% c(foco_trap$trap_id)) %>%
  select(-spp0) %>% #combine non culex species totals
  group_by(across(-total)) %>%
  summarise(total = sum(total), .groups = "drop") %>%
  trap_complete(trap = foco_trap$trap_id) %>%
  rquery::natural_join(foco_trap %>% select(trap_id, method, zone, zone2), jointype = "FULL", by = "trap_id") 

#get trap status
trap_sum = trap_stat_sum(culex_new)
trap_sum$p

ggsave(plot = trap_sum$p, filename = paste0(dir_mid_wk, "trap_status.png"))
write.csv(trap_sum$df, paste0(dir_mid_wk,"trap_status_summary.csv"))

if(update){
update_gsheet(new = culex_new,
              old = culex_database,
              by = c("trap_id", "trap_date", "spp", "zone"),
              fn_save = fn_database_culex_update,
              gkey = key_database_culex_sheet,
              gfolder = "database_archive",
              gname = "culex_sheet_database",
              col_database = names(culex_database),
              sheet = "data")
}


```

### CALC POOLS

----------------------------------------------------------------------------------------------------

Code Below: - data input: database_update (weekly data from surveillance) - data_input

```{r, calc-pools, include=FALSE}
pools = calc_pools(database_new) %>%
  wnv_s_clean(all_cols = names(.))  %>%
    mutate(zone = factor(zone, levels = zone_lvls)) %>%
    arrange(year, week, zone, spp)

write_rds(pools, paste0(dir_mid_wk,"pools.RData"))
```

### PLOT POOLS

----------------------------------------------------------------------------------------------------

```{r, plot-pools}

p_pools = plot_p_pool_pt(database_new)
p_pools

ggsave(paste0(dir_plot_wk, "pools.png"), p_pools, height = 8, width = 12, units = "in")
```

### CALC ZONE STATS

----------------------------------------------------------------------------------------------------

```{r, calc-zone-stats, include=FALSE}
culex_update = read.csv(fn_database_culex_update) %>% wnv_s_clean(silence = T)
database_update = read.csv(fn_database_update) %>% wnv_s_clean(silence = T)


# t = calc_abund(culex_update,, 
#                       grp_var = c("zone", "year", "week", "spp")) %>%
#     filter(year %in% year_filter_hx) 

all_hx = calc_all(culex_update, #input for abundance
                  database_update) %>%
  arrange(desc(year), desc(week), zone, spp) #input for pir

write.csv(all_hx, paste0(dir_output, "/zone_stats.csv"), row.names = F)

if(update){
  #this will save the 
  sheet_write(data = all_hx, 
              ss = "15SBhAI7cYMWBy-P6IOKG5mRdG2ly3x_vV_BvSJ06xEY",
              sheet =  "data" # Specify folder
            )

}

current_wk = all_hx %>%
  filter(year == year_filter,
         week == week_filter) 
  
ytd = all_hx %>%
  filter(year == year_filter,
         
         week %in% 23:week_filter) %>%
  mutate(type = "current")

hx = all_hx %>%
  filter(year %in% year_filter_hx) %>%
  mutate(type = "hx")

```

### PLOT ZONE STATS LINE

----------------------------------------------------------------------------------------------------

```{r, plot-zone-stats}
hx2 = all_hx %>%
  filter(year %in% c(2015:(year_filter-1))) %>%
  mutate(type = "hx")

ytd_hx_long = clean_long_hx(hx2, ytd, spp_keep = c("Pipiens", "Tarsalis", "All"))
write.csv(ytd_hx_long, "3_output/ytd_hx_long.csv")

# p_line = plot_hx_line(df = ytd_hx_long, 
#              text = "Larimer County WNV Surveillance", 
#              color_var = year)
# 
# p_line  = ggplotly(p_line) %>% 
#   layout(legend = list(orientation = "h",
#                        y = -0.2))
# library(htmlwidgets)
# saveWidget(p_line, file = paste0(dir_plot_wk, "p_line.html"))

```

### PLOT ZONE STATS AREA

----------------------------------------------------------------------------------------------------

```{r, plot-zone-stats-area}
ytd_hx_wk = clean_long_hx_wk(ytd, hx, 
                          #  rm_zone = c("BC", fc_zones),
                            #rm_zone = c("BC"),
                            grp_vars = c("year", "week", "zone", "spp"))

p_abund = plot_hx(ytd_hx_wk, abund, "Abundance")
p_pir = plot_hx(ytd_hx_wk, pir, "PIR")

p_vi = plot_hx(ytd_hx_wk, vi, "Vector Index") +
 geom_hline(yintercept = vi_threshold, linetype = "dashed", color = "red") +
 scale_y_continuous(limits = c(0, 1.5), breaks = seq(0, 1.5, by = 0.25))


description = paste0("*All historical data averaged from ",  
                     min(year_filter_hx), " to ", max(year_filter_hx))

library(patchwork)
p_hx_current0 = p_abund + p_pir + p_vi + 
  plot_annotation(caption = description) +
  plot_layout(#widths =c(3,3,3,1), 
              guides = "collect") & theme(legend.position = 'bottom', 
                                          legend.title = element_blank())

p_hx_current0

fn_p_hx = paste0(dir_plot_wk, "hx_plot_all.png")

ggsave(fn_p_hx, p_hx_current0, height = 8, width = 12, units = "in")


# Upload a single file to a specific folder
drive_upload(
  media = fn_p_hx,
  path = as_id(key_plots_dir),  # Replace with your folder ID
  name = basename(fn_p_hx)       # Optional: specify a new name
)


```

```{r, format-hx, include=FALSE}
#hx_wide to use for tables for report 
hx_wide = hx %>% clean_wide("hx_") 
ytd_wide = ytd %>% clean_wide("")

#combine 
df_all = hx_wide %>% left_join(ytd_wide, by = "week")

hx_abund = create_hx_report(df_all, zones = zone_lvls, prefix = "abund")
 

hx_pir = create_hx_report(df_all, zones = zone_lvls, prefix = "pir", sigfig = 4)

hx_vi = create_hx_report(df_all, zones = zone_lvls, prefix = "vi")

write.csv(hx_vi, file.path(dir_output, "table1b_hx_vi.csv"), row.names = F)
write.csv(hx_abund, file.path(dir_output, "table2b_hx_abund.csv"), row.names = F)
write.csv(hx_pir, file.path(dir_output, "table3b_hx_pir.csv"), row.names = F)
```

```{r format-tables, include=FALSE}
source("0_R/tables.R")
```

```{r, generate-report, include=FALSE}

if(update){
  suppressMessages(
  source("0_R/generate_report.R")
)
}

```

```{r}
if(push) {
  try({
  system("git add --all")
  system('git commit -m "weekly commit after report generation for the week"')
  system("git push")
})
}

```
