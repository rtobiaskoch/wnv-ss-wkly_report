---
title: "wnv-s_weekly_reports"
format: html
editor: visual
---

# Weekly Data Report

### ------------ CREATE CONFIGURATION ------------------------

DESC: defines filepaths \| \| defines settings \| \| defines filenames \| \| creates setting for plots\
INPUT: user arguments\
OUTPUT: config/config_weekly_settings/config_weekly\_\[sys.time\].RData

```{bash, create config}
Rscript config/config_weekly.R --input test --year 2025 --year_hx 2019 --week 23 --cp_threshold 500 --update F --clean F # first arg is year second is week
#add input dir arg 
#allows to easily run diff input folders
```

### -------------LOAD CONFIGURATION --------------------

DESC: loads in packages \| \| loads user functions \| \| loads config file

```{r, load config}
rm(list = ls())
getwd()

source("config/load_packages.R")

utils = list.files(path = "utils",
                   pattern = "*.R",
                   full.names = T)

purrr::walk(utils, source)

#get the most recent configuration for the run from the output of running config_weekly.R
read_latest("config/config_weekly_settings")


```

```{bash}
#git commit -a -m "weekly commit before data removal"
#git push

```

```{r, echo=FALSE, eval=TRUE}

if(clean) {

    clean_dir(dir_mid)
    clean_dir(dir_output)

  } else {NULL}

```

### -------------DATA GDRIVE LOAD -------------------------

DESC: reads in current wnv-s_database \| \| foco_trap \| \| key_rename \| \| standards data to dir_input\
**DEPENDENCIES:** googlesheets4 \| \| googledrive \| \| argparse \| \| gsheet_pull_prompt\
**INPUT:** NA\
**OUTPUT:** **\[dir_input\]/wnv-s_database.csv** \| \| **\[dir_input\]/foco_trap.csv\
\[dir_input\]/standards_input.csv** \| \| **\[dir_input\]/key_rename.csv**\

```{r}
if(update) {

#WNV DATABASE
gsheet_pull_prompt(fn_gdrive_database, "data", key_database_gsheet)
#SLEV DATABASE
gsheet_pull_prompt(fn_gdrive_database_slev, "data", key_database_gsheet_slev)
#CULEX SHEET
gsheet_pull_prompt(fn_gdrive_culex_sheet, "data", key_database_culex_sheet)

#RENAMING KEY
gsheet_pull_prompt(fn_key_rename, "Sheet1", key_rename_key)
#FOCO TRAP
gsheet_pull_prompt(fn_trap, "data", key_foco_trap)
#STANDARDS
gsheet_pull_prompt(fn_standards, "data", key_standards_gsheet)

}



df_key_rename = read.csv(fn_key_rename)  
foco_trap = read.csv(fn_trap) %>% filter(active == 1)
standards = read.csv(fn_standards)
```

### -------------DATABASE CHECK ---------------------------

```{r}
#read in database WNV
database = read.csv(fn_gdrive_database) %>% 
  wnv_s_clean(distinct_col = c("csu_id", "trap_id", "trap_date", "year", "week", "spp"))

check_data(df = database, #data you want to check
           trap = foco_trap, #data with trap_ids you want to ensure traps are available
           dir = dir_mid)
```

```{r}
#read in database SLEV
database_slev = read.csv(fn_gdrive_database_slev) %>% 
  wnv_s_clean()

check_data(df = database_slev, #data you want to check
           trap = foco_trap, #data with trap_ids you want to ensure traps are available
           dir = dir_mid)
```

```{r}
culex_database = read.csv(fn_gdrive_culex_sheet) %>% 
  wnv_s_clean()

check_data(df = culex_database, #data you want to check
           trap = foco_trap, #data with trap_ids you want to ensure traps are available
           dir = dir_mid)

```

### -------------DATASHEET CLEAN ---------------------------

Readme DESC: - reads in datasheet (pool) data from VDCI and BC, - cleans it into the proper format for the database - checks each column for errors. -

DEPENDENCIES: read_list \| \| key rename \| \| wnv_s_clean \| \| tidyverse \| \| purrr

INPUT: \[dir_input\]/datasheets\* \| \| df_key_rename

OUTPUT: \[mid_dir\]/\[fn_datasheet_clean\]\*

```{r, datasheet_clean}
datasheet_input = read_list(dir_datasheet, "*")
saveRDS(datasheet_input, fn_weekly_input_format_mid)

datasheet_clean = datasheet_input %>%
    key_rename(rename_df = df_key_rename, 
             drop_extra = T) %>%
    wnv_s_clean()

check_data(df = datasheet_clean, #data you want to check
           trap = foco_trap, #data with trap_ids you want to ensure traps are available
           dir = dir_mid,
           year_filter = year_filter, # The year the dates should be 
           week_filter = week_filter)
```

\

### --------------PCR CLEAN -----------------------------------

DESC: Reads in the pcr data from quantstudio that contains the Ct values \| \| reads in platemap data that links the platmap position with sample id\
\
DEPENDENCIES: tidyverse \| \| readxl \| \| purrr

INPUT -PCR: \[dir_input\]/pcr/\* \[dir_pcr\] \| \| PLATEMAP: \[dir_input\]/platemap/\* \[dir_platemap\] \| \| week_filter = 37 \| \| year_filter = 2024 \| \| copy_threshold = 500 \| \| rn_threshold = 34000

OUTPUT - Cq combined pcr and platemap csv \[fn_cq_out = 2_mid/y##\_w##\_platemap.csv"\]

```{r}
#TO DO make a function
#source("0_R/2_pcr_platemap_read_clean.R")

fn_path = list.files(path = dir_pcr, #
               full.names = T,
               ignore.case = T) 

cat("Reading in the PCR file: ", fn_path)

pcr_input = read_pcr(fn_path, sheet = "Results")

pcr_clean = clean_pcr(pcr_input,
                      y_pattern = "(?<=y)\\d+", #pcr filename pattern that determines year
                      w_pattern = "(?<=w)\\d+", #pcr filename pattern that determines week
                      p_pattern = "(?<=p)\\d+", #pcr filename pattern that determines plate#
                      undet_val = '55.55') #numeric value to replace "Undetermined" to ensure that the column is numeric without missing values (NA)

check_data(df = pcr_clean, 
           trap = foco_trap, 
           dir = dir_mid,
           year_filter = year_filter, 
           week_filter = week_filter)

rm(fn_path)
```

### --------------PLATEMAP CLEAN ---------------------------

DESC:\
DEPENDENCIES:\
INPUT:\
OUTPUT:

```{r}
fn_path = list.files(path = dir_platemap,
                     full.names = T,
                     ignore.case = T)  

cat("\nReading in the following platemap files:\n", 
    paste(unlist(fn_path), collapse = "\n"))

platemap_input = read_platemap(fn_path,
                      sheet = "pcr", #sheet in excel file that contains platemap
                      range = "A1:M9", #range in the sheet that contains your platemap
                      col_pattern = "(?<=x)\\d+", #pattern in col with column number
                      val_to = "csu_id" #column name you want to identify your sample
                          )
platemap_clean = clean_platemap(platemap_input,
                      y_pattern = "(?<=y)\\d+", #pcr filename pattern that determines year
                      w_pattern = "(?<=w)\\d+", #pcr filename pattern that determines week
                      p_pattern = "(?<=p)\\d+" #pcr filename pattern that determines plate#
                                )

check_data(df = platemap_clean, 
           trap = foco_trap, 
           dir = dir_mid,
           year_filter = year_filter, 
           week_filter = week_filter)

rm(fn_path)
```

### --------------MERGE PCR AND PLATEMAP -------------------------

```{r}
cq_data = merge_pcr_platemap(pcr_clean, platemap_clean) %>%
  wnv_s_clean()

check_data(df = cq_data, 
           trap = foco_trap, 
           dir = dir_mid,
           year_filter = year_filter, 
           week_filter = week_filter,
           copy_threshold = copy_threshold)


```

### -------------- STANDARDS -------------------------

```{r}
std_new = clean_standards(cq_data)

if(update){
  update_gsheet(new = std_new, 
              old = standards, 
              by = c("year", "week", "plate", "target", "sample_type"), 
              fn_save = fn_standards, 
              gkey = key_standards_gsheet,
              gfolder = "database_archive", 
              gname = "standards_archive", 
              col_database = names(standards), 
              sheet = "data" )
}

```

### -------------- BIRDS -----------------------------

```{r}
#add birds 
birds = cq_data %>%
  filter(sample_type == "bird") %>%
  mutate(year = as.double(year),
         week = as.double(week)) %>% #for natural merge
  select(csu_id, year, week, target_name, test_code, cq, copies)


if(update){
  update_gsheet(new = birds, 
              #old = standards, 
              by = c("csu_id", "year", "week"), 
              fn_save = fn_standards, 
              gkey = key_birds,
              gfolder = "database_archive", 
              gname = "birds_archive", 
              col_database = names(birds), 
              sheet = "data" )
}
rm(birds)
```

### -------------- PCR PLOT --------------------------------

```{r pcr plot}
standards = distinct_all(standards) #temporay having issues with duplicates

std = bind_rows(std_new, standards) %>%
  filter(year == year_filter)

p_std = plot_std(std)

p_pcr_wnv = plot_pcr(cq_data, 
                 virus = "WNV", 
                 pattern_2_keep = "WNV|CSU|RMRP|CDC|pos|neg",
                 copy_threshold = copy_threshold, 
                 week_filter = week_filter)



p_pcr_slev = plot_pcr(cq_data, 
                 virus = "SLEV", 
                 pattern_2_keep = "SLEV|CSU|RMRP|CDC|pos|neg",
                 copy_threshold = copy_threshold, 
                 week_filter = week_filter)

pcr_plot = p_std / p_pcr_wnv /  p_pcr_slev
pcr_plot


ggsave(paste0(dir_plots, "/pcr_plot.png"), pcr_plot,
       width = 8, height = 8, units = "in")


rm(p_std, p_pcr_wnv, p_pcr_slev)
rm(standards, cq_data)
```

### -------------- DATABASE UPDATE ---------------------------

DESC: MERGE WNV DATASHEET & PCR

```{r}

#causing duplicates
database_new = merge_4_database(datasheet_clean, cq_data,   
                            virus = "WNV",
                            copy_threshold = copy_threshold) %>%
  wnv_s_clean(distinct_col = c("csu_id", "trap_id", "year", "week", "spp"))


if(update){
  update_gsheet(new = database_new,
              old = database,
              by = c("csu_id", "trap_id", "trap_date", "spp", "zone"),
              fn_save = fn_database_update,
              gkey = key_database_gsheet,
              gfolder = "database_archive",
              gname = "wnv-s_database_archive",
              col_database = col_database,
              sheet = "data")
}


```

### -------------- MERGE SLEV DATASHEET & PCR -----------------------

```{r}

database_slev_new = merge_4_database(datasheet_clean, cq_data,   
                            virus = "SLEV",
                            copy_threshold = copy_threshold) %>%
  wnv_s_clean(distinct_col = c("csu_id", "trap_id", "year", "week", "spp"))

if(update){
  update_gsheet(new = database_slev_new,
              old = database_slev,
              by = c("csu_id", "trap_id", "trap_date", "spp", "zone"),
              fn_save = fn_database_slev_update,
              gkey = key_database_gsheet_slev,
              gfolder = "database_archive",
              gname = "slev-s_database_archive",
              col_database = names(database_slev_new),
              sheet = "data")
}

```

Code Below:

-   data_input:
    -   datasheets
    -   pcr_sheet
    -   platemap
    -   fn_func_trap: data_mid/functional_traps.csv \*to be added
-   data_output:
    -   fn_database_output
    -   unmatched traps
-   description:
    -   checks to ensure traps match existing list
    -   pulls database
    -   merges datasheet pcr and platemap data
    -   makes archive copy of database
    -   merges new data with database
    -   pushes changes from new data to database to googledrive

Code Below: -data_input

```{r}
#source("0_R/check_read_fun.R")
#source("0_R/weekly_data_input.R")


weekly_data_format = clean_4_weekly_input(fn_weekly_input_format_mid, database_new)

write.csv(weekly_data_format, fn_weekly_input_format, row.names = F, na = "")
```

### -------------- CLEAN APP SPECIES (TRAP) DATA ————————————–

DESC:\
DEPENDENCIES:\
INPUT:\
OUTPUT:

```{r}
#TO DO add 
all_spp0 = read_list(dir_all_spp, "*") 
all_spp = all_spp0 %>%
  key_rename(rename_df = df_key_rename,
             drop_extra = T) %>%
  mutate(spp0 = spp) %>% #keep original
  wnv_s_clean()

write.csv(all_spp, file.path(dir_mid, "all_species_active_trap.csv"))

```

### -------------- COMPLETE TRAP DATA —————————————————–

DESC:\
DEPENDENCIES:\
INPUT:\
OUTPUT:

```{r}
#keep only traps in routine active traps from surveillance
culex_new = all_spp %>%
  filter(trap_id %in% foco_trap$trap_id) %>%
  select(-spp0) %>% #combine non culex species totals
  group_by(across(-total)) %>%
  summarise(total = sum(total), .groups = "drop") %>%
  trap_complete(trap = foco_trap$trap_id) %>%
  rquery::natural_join(foco_trap %>% select(trap_id, method, zone, zone2), jointype = "FULL", by = "trap_id")
  
#dunno where this function went
# zones_check = check_zone(foco_trap, culex_new)
# write.csv(zones_check, file.path(dir_mid, "zones_check.csv"))

#get trap status
trap_sum = trap_stat_sum(culex_new)
trap_sum$p
write.csv(trap_sum$df, file.path(dir_mid, "trap_status_summary.csv"))



if(update){
  update_gsheet(new = culex_new,
              old = culex_database,
              by = c("trap_id", "trap_date", "spp", "zone"),
              fn_save = fn_database_culex_update,
              gkey = key_database_culex_sheet,
              gfolder = "database_archive",
              gname = "culex_sheet_database",
              col_database = names(culex_database),
              sheet = "data")
}


```

\

### -------------- CALC ABUNDANCE —————————————————–

DESC:\
DEPENDENCIES:\
INPUT:\
OUTPUT:

```{r}
#group by zone
abund0 = calc_abund(culex_new, 
                  grp_var = c("zone", "year", "week", "spp"))

#group FC
abund_fc = calc_abund(culex_new, 
                  grp_var = c("zone2", "year", "week", "spp"))

#group all spp
abund_all = calc_abund(culex_new, 
                  grp_var = c("zone", "year", "week"))

#group all FC
abund_all_fc = calc_abund(culex_new, 
                  grp_var = c("zone2", "year", "week"))

abund = bind_rows(abund0, abund_all, 
                  abund_fc, abund_all_fc) %>%
  distinct_all() %>%
  wnv_s_clean(all_cols = names(.)) %>%
  arrange(year, week, zone, spp)

check_data(df = abund,
           trap = foco_trap, 
           dir = dir_mid,
           year_filter = year_filter, 
           week_filter = week_filter
           )


write.csv(abund, fn_abund_out, row.names = F)


write.csv(culex_new, file.path(dir_mid, "abund_trap.csv"))
write.csv(abund0$trap, file.path(dir_mid, "abund_trap.csv"))
write.csv(abund0$grp, file.path(dir_mid, "abund.csv"))

rm(abund0, abund_fc, abund_all, abund_all_fc)
```

### -------------- CALC POOLS—————————————————– 

Code Below: - data input: database_update (weekly data from surveillance) - data_input

```{r}
pools = calc_pools(database_new) %>%
  wnv_s_clean(all_cols = names(.))

skimr::skim(pools)

```

### -------------- CALC PIR —————————————————–

Code Below:

Data Input:\
-output from abundance.R\
-data_input file:\
\
Data Output:\
-yYYYY_wWW_data_update\
\
Description:

\- calculates pooled Infection Rate and Vector Index\
-calculates all spp for each zone (TBD 24-07-16)

this script will drop weeks that only have a gravid trap. because gravid traps aren't used in abundance calculations

```{r}
#add complete for the different zones
pir0 = calc_pir(database_new,
               grp_var = c("zone", "year", "week", "spp"))

pir_fc = calc_pir(database_new , 
                   grp_var = c("zone2", "year","spp", "week"))

pir_all = calc_pir(database_new , 
                   grp_var = c("zone", "year", "week"))

pir_fc_all = calc_pir(database_new, 
                   grp_var = c("zone2", "year", "week"))

pir = bind_rows(pir0, pir_all,
          pir_fc, pir_fc_all) %>% 
  distinct_all() %>%
  wnv_s_clean(all_cols = names(pir0)) %>%
  arrange(year, week, zone, spp)

write.csv(pir, file.path(dir_mid, "pir.csv"))

rm(pir0, pir_all, pir_fc, pir_fc_all)
```

### -------------- CALC VI —————————————————–

```{r}
vi = calc_vi(abund, pir,
             by = grp_vars) %>%
   wnv_s_clean(rm_col = c("trap_status"))

```

### -------------- CALC ZONE STATS —————————————————–

```{r}
culex_update = read.csv(fn_database_culex_update) %>% wnv_s_clean(silence = T)
database_update = read.csv(fn_database_update) %>% wnv_s_clean(silence = T)


# t = calc_abund(culex_update,, 
#                       grp_var = c("zone", "year", "week", "spp")) %>%
#     filter(year %in% year_filter_hx) 

all_hx = calc_all(culex_update, #input for abundance
                  database_update)  #input for pir

current_wk = all_hx %>%
  filter(year == year_filter,
         week == week_filter)
  
ytd = all_hx %>%
  filter(year == year_filter)

hx = all_hx %>%
  filter(year %in% year_filter_hx) 

#hx_wide to use for tables for report
hx_wide = hx %>% clean_wide("hx_")
ytd_wide = ytd %>% clean_wide("")

#combine
df_all =  hx_wide %>% left_join(ytd_wide, by = "week")

hx_abund = create_hx_report(df_all, 
              zones = zone_lvls, 
              prefix = "abund")


hx_pir= create_hx_report(df_all, 
              zones = zone_lvls, 
              prefix = "pir")

```

\

```{r}
source("0_R/tables.R")
```

Before generating tables and report make sure all the stuff is correct\

```{r}
#source("0_R/QC_report.R")
```

```{r}
suppressMessages(
  source("0_R/generate_report.R")
)
```
