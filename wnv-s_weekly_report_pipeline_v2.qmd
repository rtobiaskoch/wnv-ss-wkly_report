---
title: "wnv-s_weekly_reports"
format: html
editor: visual
---

# Weekly Data Report

### ------------ CREATE CONFIGURATION ------------------------

DESC: defines filepaths \| \| defines settings \| \| defines filenames \| \| creates setting for plots\
INPUT: user arguments\
OUTPUT: config/config_weekly_settings/config_weekly\_\[sys.time\].RData

```{bash, create config}
Rscript config/config_weekly.R --input w26 --year 2025 --year_hx 2019 --week 26 --cp_threshold 500 --update T --clean F # first arg is year second is week
#add input dir arg 
#allows to easily run diff input folders
```

```{r}
knitr::opts_chunk$set(
  echo = TRUE,       # Keep code visible
  results = 'hide',  # Hide printed output
  message = FALSE,   # Hide messages
  warning = FALSE,   # Hide warnings
  fig.show = 'show'  # Keep plots visible
)
```

### -------------LOAD CONFIGURATION --------------------

DESC: loads in packages \| \| loads user functions \| \| loads config file

```{r, load config}
rm(list = ls())
getwd()

source("config/load_packages.R")

utils = list.files(path = "utils",
                   pattern = "*.R",
                   full.names = T)

purrr::walk(utils, source)

#get the most recent configuration for the run from the output of running config_weekly.R
read_latest("config/config_weekly_settings")


```

```{bash}
#git commit -a -m "weekly commit before data removal"
#git push

```

```{r}

# if(clean) {
# 
#     clean_dir(dir_mid)
#     clean_dir(dir_output)
# 
#   } else {NULL}

```

### -------------DATA GDRIVE LOAD -------------------------

DESC: reads in current wnv-s_database \| \| foco_trap \| \| key_rename \| \| standards data to dir_input\
**DEPENDENCIES:** googlesheets4 \| \| googledrive \| \| argparse \| \| gsheet_pull_prompt\
**INPUT:** NA\
**OUTPUT:** **\[dir_input\]/wnv-s_database.csv** \| \| **\[dir_input\]/foco_trap.csv\
\[dir_input\]/standards_input.csv** \| \| **\[dir_input\]/key_rename.csv**\

```{r}
if(update) {

#WNV DATABASE
gsheet_pull_prompt(fn_gdrive_database, "data", key_database_gsheet)
#SLEV DATABASE
gsheet_pull_prompt(fn_gdrive_database_slev, "data", key_database_gsheet_slev)
#CULEX SHEET
gsheet_pull_prompt(fn_gdrive_culex_sheet, "data", key_database_culex_sheet)

#RENAMING KEY
gsheet_pull_prompt(fn_key_rename, "Sheet1", key_rename_key)
#FOCO TRAP
gsheet_pull_prompt(fn_trap, "data", key_foco_trap)
#STANDARDS
gsheet_pull_prompt(fn_standards, "data", key_standards_gsheet)

}



df_key_rename = read.csv(fn_key_rename)  
foco_trap = read.csv(fn_trap) %>% filter(active == 1)
standards = read.csv(fn_standards)
```

### -------------DATABASE CHECK ---------------------------

```{r, include=FALSE}
#read in database WNV
database = read.csv(fn_gdrive_database) %>% 
  wnv_s_clean(distinct_col = c("csu_id", "trap_id", "trap_date", "year", "week", "spp"))

check_data(df = database, #data you want to check
           trap = foco_trap, #data with trap_ids you want to ensure traps are available
           dir = dir_mid)
```

```{r, include=FALSE}
#read in database SLEV
database_slev = read.csv(fn_gdrive_database_slev) %>% 
  wnv_s_clean()

check_data(df = database_slev, #data you want to check
           trap = foco_trap, #data with trap_ids you want to ensure traps are available
           dir = dir_mid)
```

```{r, include=FALSE}
culex_database = read.csv(fn_gdrive_culex_sheet) %>%
 # filter(trap_status != "missing") %>% #TO DO add to wnv_s_clean and remove
  wnv_s_clean()

check_data(df = culex_database, #data you want to check
           trap = foco_trap, #data with trap_ids you want to ensure traps are available
           dir = dir_mid)

```

### -------------DATASHEET CLEAN ---------------------------

Readme DESC: - reads in datasheet (pool) data from VDCI and BC, - cleans it into the proper format for the database - checks each column for errors. -

DEPENDENCIES: read_list \| \| key rename \| \| wnv_s_clean \| \| tidyverse \| \| purrr

INPUT: \[dir_input\]/datasheets\* \| \| df_key_rename

OUTPUT: \[mid_dir\]/\[fn_datasheet_clean\]\*

```{r, include=FALSE}
datasheet_input = read_list(dir_datasheet, "*")
saveRDS(datasheet_input, fn_weekly_input_format_mid)

datasheet_clean = datasheet_input %>%
    key_rename(rename_df = df_key_rename, 
             drop_extra = T) %>%
    wnv_s_clean()

check_data(df = datasheet_clean, #data you want to check
           trap = foco_trap, #data with trap_ids you want to ensure traps are available
           dir = dir_mid,
           year_filter = year_filter, # The year the dates should be 
           week_filter = week_filter)
```

\

### --------------PCR CLEAN -----------------------------------

DESC: Reads in the pcr data from quantstudio that contains the Ct values \| \| reads in platemap data that links the platmap position with sample id\
\
DEPENDENCIES: tidyverse \| \| readxl \| \| purrr

INPUT -PCR: \[dir_input\]/pcr/\* \[dir_pcr\] \| \| PLATEMAP: \[dir_input\]/platemap/\* \[dir_platemap\] \| \| week_filter = 37 \| \| year_filter = 2024 \| \| copy_threshold = 500 \| \| rn_threshold = 34000

OUTPUT - Cq combined pcr and platemap csv \[fn_cq_out = 2_mid/y##\_w##\_platemap.csv"\]

```{r, include=F}
#TO DO make a function
#source("0_R/2_pcr_platemap_read_clean.R")

fn_path = list.files(path = dir_pcr, #
               full.names = T,
               ignore.case = T) 

cat("Reading in the PCR file: ", fn_path)

pcr_input = read_pcr(fn_path, sheet = "Results")

pcr_clean = clean_pcr(pcr_input,
                      y_pattern = "(?<=y)\\d+", #pcr filename pattern that determines year
                      w_pattern = "(?<=w)\\d+", #pcr filename pattern that determines week
                      p_pattern = "(?<=p)\\d+", #pcr filename pattern that determines plate#
                      undet_val = '55.55') #numeric value to replace "Undetermined" to ensure that the column is numeric without missing values (NA)

check_data(df = pcr_clean, 
           trap = foco_trap, 
           dir = dir_mid,
           year_filter = year_filter, 
           week_filter = week_filter)

write_rds(pcr_clean, paste0(dir_mid_wk, "pcr_clean.RData"))
rm(fn_path)
```

### --------------PLATEMAP CLEAN ---------------------------

DESC:\
DEPENDENCIES:\
INPUT:\
OUTPUT:

```{r, include=FALSE}
fn_path = list.files(path = dir_platemap,
                     full.names = T,
                     ignore.case = T)  

cat("\nReading in the following platemap files:\n", 
    paste(unlist(fn_path), collapse = "\n"))

platemap_input = read_platemap(fn_path,
                      sheet = "pcr", #sheet in excel file that contains platemap
                      range = "A1:M9", #range in the sheet that contains your platemap
                      col_pattern = "(?<=x)\\d+", #pattern in col with column number
                      val_to = "csu_id" #column name you want to identify your sample
                          )
platemap_clean = clean_platemap(platemap_input,
                      y_pattern = "(?<=y)\\d+", #pcr filename pattern that determines year
                      w_pattern = "(?<=w)\\d+", #pcr filename pattern that determines week
                      p_pattern = "(?<=p)\\d+" #pcr filename pattern that determines plate#
                                )

check_data(df = platemap_clean, 
           trap = foco_trap, 
           dir = dir_mid,
           year_filter = year_filter, 
           week_filter = week_filter)

write_rds(platemap_clean, paste0(dir_mid_wk, "platemap_clean.RData"))

rm(fn_path)
```

### --------------MERGE PCR AND PLATEMAP -------------------------

```{r, include=FALSE}
cq_data = merge_pcr_platemap(pcr_clean, platemap_clean) %>%
  wnv_s_clean()

check_data(df = cq_data, 
           trap = foco_trap, 
           dir = dir_mid,
           year_filter = year_filter, 
           week_filter = week_filter,
           copy_threshold = copy_threshold)

write_rds(cq_data, paste0(dir_mid_wk, "cq_data.RData"))
```

### -------------- STANDARDS -------------------------

```{r, include=FALSE}
std_new = clean_standards(cq_data)

if(update){
  update_gsheet(new = std_new, 
              old = standards, 
              by = c("year", "week", "plate", "target", "sample_type", "csu_id"), 
              fn_save = fn_standards, 
              gkey = key_standards_gsheet,
              gfolder = "database_archive", 
              gname = "standards_archive", 
              col_database = names(standards), 
              sheet = "data" )
}

```

### -------------- BIRDS -----------------------------

```{r, include=FALSE}
#add birds 
birds = cq_data %>%
  filter(sample_type == "bird") %>%
  mutate(year = as.double(year),
         week = as.double(week)) %>% #for natural merge
  select(csu_id, year, week, target_name, test_code, cq, copies)



if(update){
  update_gsheet(new = birds, 
              #old = standards, 
              by = c("csu_id", "year", "week", "target_name"), 
              fn_save = fn_bird_output, 
              gkey = key_birds,
              gfolder = "database_archive", 
              gname = "birds_archive", 
              col_database = names(birds), 
              sheet = "data" )
}
#rm(birds)
```

### -------------- PCR PLOT --------------------------------

```{r pcr plot}
standards = distinct_all(standards) #temporay having issues with duplicates

std = bind_rows(std_new, standards) %>%
  filter(year == year_filter)

p_std = plot_std(std)

p_pcr_wnv = plot_pcr(cq_data, 
                 virus = "WNV", 
                 pattern_2_keep = "WNV|CSU|RMRP|CDC|pos|neg",
                 copy_threshold = copy_threshold, 
                 week_filter = week_filter)



p_pcr_slev = plot_pcr(cq_data, 
                 virus = "SLEV", 
                 pattern_2_keep = "SLEV|CSU|RMRP|CDC|pos|neg",
                 copy_threshold = copy_threshold, 
                 week_filter = week_filter)

pcr_plot = p_std / p_pcr_wnv /  p_pcr_slev
pcr_plot


ggsave(paste0(dir_plot_wk, "pcr_plot.png"), pcr_plot,
       width = 8, height = 8, units = "in")


rm(p_std, p_pcr_wnv, p_pcr_slev)
#rm(standards, cq_data)
```

### -------------- DATABASE UPDATE ---------------------------

DESC: MERGE WNV DATASHEET & PCR

```{r, include=FALSE}

#causing duplicates
database_new = merge_4_database(datasheet_clean, cq_data,   
                            virus = "WNV",
                            copy_threshold = copy_threshold) %>%
  wnv_s_clean(distinct_col = c("csu_id", "trap_id", "year", "week", "spp"))


write_rds(database_new, paste0(dir_mid_wk, "database_new.RData"))

if(update){
  update_gsheet(new = database_new,
              old = database,
              by = c("csu_id", "trap_id", "trap_date", "spp", "zone"),
              fn_save = fn_database_update,
              gkey = key_database_gsheet,
              gfolder = "database_archive",
              gname = "wnv-s_database_archive",
              col_database = col_database,
              sheet = "data")
}


```

### -------------- MERGE SLEV DATASHEET & PCR -----------------------

```{r, include=FALSE}

database_slev_new = merge_4_database(datasheet_clean, cq_data,   
                            virus = "SLEV",
                            copy_threshold = copy_threshold) %>%
  wnv_s_clean(distinct_col = c("csu_id", "trap_id", "year", "week", "spp"))

if(update){
  update_gsheet(new = database_slev_new,
              old = database_slev,
              by = c("csu_id", "trap_id", "trap_date", "spp", "zone"),
              fn_save = fn_database_slev_update,
              gkey = key_database_gsheet_slev,
              gfolder = "database_archive",
              gname = "slev-s_database_archive",
              col_database = names(database_slev_new),
              sheet = "data")
}

```

Code Below:

-   data_input:
    -   datasheets
    -   pcr_sheet
    -   platemap
    -   fn_func_trap: data_mid/functional_traps.csv \*to be added
-   data_output:
    -   fn_database_output
    -   unmatched traps
-   description:
    -   checks to ensure traps match existing list
    -   pulls database
    -   merges datasheet pcr and platemap data
    -   makes archive copy of database
    -   merges new data with database
    -   pushes changes from new data to database to googledrive

Code Below: -data_input

```{r, include=FALSE}
#source("0_R/check_read_fun.R")
#source("0_R/weekly_data_input.R")


weekly_data_format = clean_4_weekly_input(fn_weekly_input_format_mid, database_new)

write.csv(weekly_data_format, fn_weekly_input_format, row.names = F, na = "")
```

### -------------- CLEAN APP SPECIES (TRAP) DATA ————————————–

DESC:\
DEPENDENCIES:\
INPUT:\
OUTPUT:

```{r, include=FALSE}
#TO DO add 
all_spp0 = read_list(dir_all_spp, "*") 
all_spp = all_spp0 %>%
  key_rename(rename_df = df_key_rename,
             drop_extra = T) %>%
  mutate(spp0 = spp) %>% #keep original
  wnv_s_clean()

write_rds(all_spp, paste0(dir_mid_wk,"all_species_active_trap.RData"))

```

### -------------- COMPLETE TRAP DATA —————————————————–

DESC:\
DEPENDENCIES:\
INPUT:\
OUTPUT:

```{r, include=FALSE}
#keep only traps in routine active traps from surveillance
culex_new = all_spp %>%
  filter(trap_id %in% foco_trap$trap_id) %>%
  select(-spp0) %>% #combine non culex species totals
  group_by(across(-total)) %>%
  summarise(total = sum(total), .groups = "drop") %>%
  trap_complete(trap = foco_trap$trap_id) %>%
  rquery::natural_join(foco_trap %>% select(trap_id, method, zone, zone2), jointype = "FULL", by = "trap_id")
  

#mosquito count per trap
ggplot(culex_database %>% filter(!is.na(spp)), aes(total, color = spp, fill = spp)) +
  geom_histogram(bins = 50, alpha = 0.5, position = "identity") +
  theme_classic() +
  facet_wrap(~zone2)


#get trap status
trap_sum = trap_stat_sum(culex_new)
trap_sum$p
ggsave(plot = trap_sum$p, filename = paste0(dir_mid_wk, "trap_status.png"))
write.csv(trap_sum$df, paste0(dir_mid_wk,"trap_status_summary.csv"))



if(update){
  update_gsheet(new = culex_new,
              old = culex_database,
              by = c("trap_id", "trap_date", "spp", "zone"),
              fn_save = fn_database_culex_update,
              gkey = key_database_culex_sheet,
              gfolder = "database_archive",
              gname = "culex_sheet_database",
              col_database = names(culex_database),
              sheet = "data")
}


```

### -------------- CALC POOLS—————————————————–

Code Below: - data input: database_update (weekly data from surveillance) - data_input

```{r, include=FALSE}
pools = calc_pools(database_new) %>%
  wnv_s_clean(all_cols = names(.))  %>%
    mutate(zone = factor(zone, levels = zone_lvls)) %>%
    arrange(year, week, zone, spp)

write_rds(pools, paste0(dir_mid_wk,"pools.RData"))
```

### --------------PLOT POOLS—————————————————–

```{r}
p_pools <- plot_pools(pools)
p_pools
  
ggsave(paste0(dir_plot_wk, "pools.png"), p_pools, height = 8, width = 12, units = "in")
```

### -------------- CALC ZONE STATS —————————————————–

```{r, include=FALSE}
culex_update = read.csv(fn_database_culex_update) %>% wnv_s_clean(silence = T)
database_update = read.csv(fn_database_update) %>% wnv_s_clean(silence = T)


# t = calc_abund(culex_update,, 
#                       grp_var = c("zone", "year", "week", "spp")) %>%
#     filter(year %in% year_filter_hx) 

all_hx = calc_all(culex_update, #input for abundance
                  database_update)%>%
  arrange(year, week, zone, spp)  #input for pir

write.csv(all_hx, paste0(dir_mid_wk, "zone_stats.csv"), row.names = F)

#how many zones are missing data
# na_data = all_hx %>% 
#   filter(year != year_filter) %>%
#   filter(!zone %in% c("FC", "BC")) %>%
#   filter(is.na(abund)) %>% 
#   filter(spp == "All") %>%
#   group_by(year, week, zone) %>% 
#   count() %>%
#   ggplot(aes(x = week, y = n, fill = zone)) +
#   geom_col() +
#   facet_wrap(~year) +
#   theme_classic() +
#   ggtitle("Zones with Missing Data by Week")

# ggsave(filename = paste0(dir_plots, "/missing_weeks_hx.png"))

current_wk = all_hx %>%
  filter(year == year_filter,
         week == week_filter) 
  
ytd = all_hx %>%
  filter(year == year_filter,
         week %in% 23:week_filter) %>%
  mutate(type = "current")

hx = all_hx %>%
  filter(year %in% year_filter_hx) %>%
  mutate(type = "hx")

```

### -------------- PLOT ZONE STATS HIST —————————————————–

```{r}
  hx %>% 
   filter(spp == "All" & zone %in% c("BE", "LV", "FC")) %>%
ggplot(aes(abund, color = zone2, fill = zone2)) +
     geom_density(alpha = 0.4, position = "identity") +
     theme_classic()
 
 
  hx %>% 
   filter(spp == "All" & zone %in% c("BE", "LV", "FC")) %>%
ggplot(aes(vi, color = zone2, fill = zone2)) +
     geom_density(alpha = 0.4, position = "identity") +
     theme_classic()
```

### -------------- PLOT ZONE STATS LINE —————————————————–

```{r}
hx2 = all_hx %>%
  filter(year %in% c(2015:(year_filter-1))) %>%
  mutate(type = "hx")

ytd_hx_long = clean_long_hx(hx2, ytd)

p_line = plot_hx_line(df = ytd_hx_long, 
             text = "Larimer County WNV Surveillance", 
             color_var = year)

p_line  = ggplotly(p_line) %>% 
  layout(legend = list(orientation = "h",
                       y = -0.2))
library(htmlwidgets)
saveWidget(p_line, file = paste0(dir_plot_wk, "p_line.html"))

```

### -------------- PLOT ZONE STATS AREA —————————————————–

```{r}
ytd_hx_wk = clean_long_hx_wk(ytd, hx, 
                          #  rm_zone = c("BC", fc_zones),
                            rm_zone = c("BC"),
                            grp_vars = c("year", "week", "zone", "spp"))

p_abund = plot_hx(ytd_hx_wk, abund, "Abundance")
p_pir = plot_hx(ytd_hx_wk, pir, "PIR")

p_vi = plot_hx(ytd_hx_wk, vi, "Vector Index") +
 geom_hline(yintercept = vi_threshold, linetype = "dashed", color = "red") +
 scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.25))


description = paste0("*All historical data averaged from ",  
                     min(year_filter_hx), " to ", max(year_filter_hx))

library(patchwork)
p_hx_current0 = p_abund + p_pir + p_vi + 
  plot_annotation(caption = description) +
  plot_layout(#widths =c(3,3,3,1), 
              guides = "collect") & theme(legend.position = 'bottom', 
                                          legend.title = element_blank())

p_hx_current0

fn_p_hx = paste0(dir_plot_wk, "hx_plot_all.png")

ggsave(fn_p_hx, p_hx_current0, height = 8, width = 12, units = "in")


# Upload a single file to a specific folder
drive_upload(
  media = fn_p_hx,
  path = as_id(key_plots_dir),  # Replace with your folder ID
  name = "new_filename.csv"        # Optional: specify a new name
)


```

\

```{r, include=FALSE}
#hx_wide to use for tables for report 
hx_wide = hx %>% clean_wide("hx_") 
ytd_wide = ytd %>% clean_wide("")

#combine 
df_all = hx_wide %>% left_join(ytd_wide, by = "week")

hx_abund = create_hx_report(df_all, zones = zone_lvls, prefix = "abund")
 

hx_pir = create_hx_report(df_all, zones = zone_lvls, prefix = "pir", sigfig = 4)

hx_vi = create_hx_report(df_all, zones = zone_lvls, prefix = "vi")

write.csv(hx_vi, file.path(dir_output, "table1b_hx_vi.csv"), row.names = F)
write.csv(hx_abund, file.path(dir_output, "table2b_hx_abund.csv"), row.names = F)
write.csv(hx_pir, file.path(dir_output, "table3b_hx_pir.csv"), row.names = F)
```

```{r, include=FALSE}
source("0_R/tables.R")
```

Before generating tables and report make sure all the stuff is correct\

```{r}
#source("0_R/QC_report.R")
```

```{r}
suppressMessages(
  source("0_R/generate_report.R")
)
```
